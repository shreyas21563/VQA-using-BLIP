\section{Commentary}
\label{sec:formatting}

\subsection{Ritwik Harit}
Through our study, we have used a comprehensive framework to evaluate the performance of the BLIP model across different datasets. We have conducted a detailed exploratory data analysis to showcase the datasets and their characteristics. Moreover, the Results and Analysis section highlights the areas where our model performs well and where it needs improvement. As shown by poor accuracy on the DAQUAR dataset, the BLIP model is unable to predict the exact answers in many cases. However, the high BERT scores show that there is a high degree of semantic similarity between the predictions of the BLIP model and the ground truth answers. Such specialised metrics for evaluation provide us with better insights into the performance of the model, which have not been used in the original paper.
\subsection{Shreyas Kabra}
Our task was to evaluate the performance of the BLIP model across different datasets including the one it was trained on. As shown by the results, the model performs exceptionally well for the VQA v2 dataset, however, its performance decreases sharply on the DAQUAR dataset. However, the value of the BERT score is still very good on the DAQUAR dataset, implying that the model is able to produce results which are semantically similar to the ground truth. Moreover, good results on the VQA datasets suggest that the model is trained well and gives precise results for these datasets, as shown by high accuracy. However, such precision is lost when the DAQUAR dataset is used, as shown by the low accuracy score. 
\subsection{Vasan Vohra}
In this project, we looked at how to utilize BLIP on different datasets to bring out the best outcomes in the VQA, i.e., Visual Question Answering. The task is to extract an answer from a given image based on the question provided. The first step was an extensive dataset analysis to understand the features. The next step is to understand the different metrics that could be utilized to evaluate the results. We have provided an explanation of why a particular metric is better than another. The low accuracy and BLEU score in DAQUAR make it clear that the model cannot give exact answers. The high BERT score in the same means that the meaning of the outcomes of the BLIP is closer to the result we need to reach. The overall results on the VQA dataset were better than DAQUAR. Using better and more explanatory evaluations in the model outputs makes this project different from the already defined papers.


%------------------------------------------------------------------------





